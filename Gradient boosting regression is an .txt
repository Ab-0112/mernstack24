Gradient boosting regression is an ML strategy that focuses on the provision of predictions, especially with numerical data. Say you have a problem where you want to predict something like house prices, and you have features like size and location. 
Here is a simple way to understand how gradient boosting regression works. 
Start Simple: It starts with a very simple model, basically predicting the target variable's (e.g., house price) average.
 Learn from Mistakes: Then it learns from the errors made by this simple model, which is how much each of its predictions differed from the observed value. And then it trains a new model to predict these errors (residuals). This new model is in a sense more set out for repairing the mistakes made by the first model.
 Add It Up: The new model's predictions are added to the original model's prediction to make it as accurate as possible. Now Repeat the Same Process: This process of learning from one's mistakes and adding a new model to correct them is repeated many times. Everyone attempts to correct the errors of past combined models. Final Prediction: The final prediction is made by adding all these models after many cycles. The resulting prediction is quite accurate because the method keeps getting better, with the focus on the mistakes of the previous models. In simple terms, gradient boosting regression works by adding up a lot of weak models into a strong predictor, in that each model tries to correct the errors the previous one made.